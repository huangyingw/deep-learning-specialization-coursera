# coding: utf-8

# # Table of Contents
#  <p><div class="lev1 toc-item"><a href="#Optimization-Algorithms" data-toc-modified-id="Optimization-Algorithms-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Optimization Algorithms</a></div><div class="lev2 toc-item"><a href="#Mini-batch-gradient-descent" data-toc-modified-id="Mini-batch-gradient-descent-11"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Mini-batch gradient descent</a></div><div class="lev2 toc-item"><a href="#Understanding-mini-batch-gradient-descent" data-toc-modified-id="Understanding-mini-batch-gradient-descent-12"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Understanding mini-batch gradient descent</a></div><div class="lev2 toc-item"><a href="#Exponentially-weighted-averages" data-toc-modified-id="Exponentially-weighted-averages-13"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>Exponentially weighted averages</a></div><div class="lev2 toc-item"><a href="#Understanding-exponentially-weighted-averages" data-toc-modified-id="Understanding-exponentially-weighted-averages-14"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>Understanding exponentially weighted averages</a></div><div class="lev2 toc-item"><a href="#Bias-correction-in-exponentially-weighted-averages" data-toc-modified-id="Bias-correction-in-exponentially-weighted-averages-15"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>Bias correction in exponentially weighted averages</a></div><div class="lev2 toc-item"><a href="#Gradient-descent-with-momentum" data-toc-modified-id="Gradient-descent-with-momentum-16"><span class="toc-item-num">1.6&nbsp;&nbsp;</span>Gradient descent with momentum</a></div><div class="lev2 toc-item"><a href="#RMSprop" data-toc-modified-id="RMSprop-17"><span class="toc-item-num">1.7&nbsp;&nbsp;</span>RMSprop</a></div><div class="lev2 toc-item"><a href="#Adam-optimization-algorithm" data-toc-modified-id="Adam-optimization-algorithm-18"><span class="toc-item-num">1.8&nbsp;&nbsp;</span>Adam optimization algorithm</a></div><div class="lev2 toc-item"><a href="#Learning-rate-decay" data-toc-modified-id="Learning-rate-decay-19"><span class="toc-item-num">1.9&nbsp;&nbsp;</span>Learning rate decay</a></div><div class="lev2 toc-item"><a href="#The-problem-of-local-optima" data-toc-modified-id="The-problem-of-local-optima-110"><span class="toc-item-num">1.10&nbsp;&nbsp;</span>The problem of local optima</a></div>

# # Optimization Algorithms

# ## Mini-batch gradient descent

# ![](https://i.imgur.com/h3SqqNK.png)

# ![](https://i.imgur.com/K23a5Gi.png)

# ## Understanding mini-batch gradient descent

# ![](https://i.imgur.com/wW3Txuu.png)

# ![](https://i.imgur.com/evd9hBu.png)

# ![](https://i.imgur.com/EVfcDPj.png)

# ## Exponentially weighted averages

# ![](https://i.imgur.com/b1UZZ4Q.png)

# ![](https://i.imgur.com/mqtOda0.png)

# ## Understanding exponentially weighted averages

# ![](https://i.imgur.com/JxgOHmA.png)

# ![](https://i.imgur.com/j80bD9K.png)

# ## Bias correction in exponentially weighted averages
#

# ![](https://i.imgur.com/CT94Pul.png)

# ## Gradient descent with momentum

# ![](https://i.imgur.com/p8IPNT6.png)
# ![](https://i.imgur.com/KiOMl7d.png)

# ## RMSprop

# ![](https://i.imgur.com/9x4WBpC.png)

# ## Adam optimization algorithm

# ![](https://i.imgur.com/TDk4C43.png)
#
# ![](https://i.imgur.com/4NlC7mp.png)

# ## Learning rate decay

# ![](https://i.imgur.com/LQ0ZWwZ.png)
# ![](https://i.imgur.com/YxyiEOp.png)
# ![](https://i.imgur.com/09caydm.png)

# ## The problem of local optima

# ![](https://i.imgur.com/Lr4ADeO.png)

# ![](https://i.imgur.com/gzSH2rA.png)
